{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of districts: 37\n",
      "Districts: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38]\n",
      "First district:\n",
      "        DATE  DISTRICT  WEATHER_CONDITION  SPECIAL_DAY  DAY_OF_WEEK_NUM  \\\n",
      "0 2024-01-01         1                  1            1                1   \n",
      "1 2024-01-03         1                  2            0                3   \n",
      "2 2024-01-04         1                  2            0                4   \n",
      "3 2024-01-05         1                  2            0                5   \n",
      "4 2024-01-06         1                  1            0                6   \n",
      "\n",
      "   POPULATION  RAILWAY_STATIONS  BUS_STATIONS  CONGESTION  \n",
      "0      344868                 5           678       0.213  \n",
      "1      344868                 5           678       0.243  \n",
      "2      344868                 5           678       0.234  \n",
      "3      344868                 5           678       0.238  \n",
      "4      344868                 5           678       0.246  \n",
      "shape: (343, 9)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"data/all_data_merged.csv\", parse_dates=[\"DATE\"])\n",
    "\n",
    "df = df.sort_values([\"DISTRICT\",\"DATE\"])\n",
    "\n",
    "district_groups = {d: g.reset_index(drop=True)\n",
    "                   for d, g in df.groupby(\"DISTRICT\")}\n",
    "\n",
    "print(f\"Number of districts: {len(district_groups)}\")\n",
    "print(f\"Districts: {list(district_groups.keys())}\")\n",
    "print(f\"First district:\\n{district_groups[1].head()}\")\n",
    "print(\"shape:\", district_groups[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def make_sequences(group, seq_len=10):\n",
    "    X_raw = group[[\"WEATHER_CONDITION\",\"SPECIAL_DAY\",\"DAY_OF_WEEK_NUM\",\n",
    "                   \"POPULATION\",\"RAILWAY_STATIONS\",\"BUS_STATIONS\"]].values\n",
    "    y_raw = group[\"CONGESTION\"].values.reshape(-1,1)\n",
    "    \n",
    "    sx, sy = MinMaxScaler(), MinMaxScaler()\n",
    "    Xs, ys = sx.fit_transform(X_raw), sy.fit_transform(y_raw)\n",
    "    \n",
    "    Xs_seq, ys_seq = [], []\n",
    "    for i in range(len(Xs) - seq_len):\n",
    "        Xs_seq.append(Xs[i:i+seq_len])\n",
    "        ys_seq.append(ys[i+seq_len])\n",
    "    return np.array(Xs_seq), np.array(ys_seq), sy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMRegressor(nn.Module):\n",
    "    def __init__(self, n_features, hidden_dim=32, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(n_features, hidden_dim, n_layers, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h, _ = self.lstm(x)\n",
    "        return self.out(h[:, -1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_weights(model):\n",
    "    return np.concatenate([p.detach().cpu().numpy().ravel() for p in model.parameters()])\n",
    "\n",
    "def unflatten_weights(model, vec):\n",
    "    idx = 0\n",
    "    for p in model.parameters():\n",
    "        numel = p.numel()\n",
    "        chunk = vec[idx:idx+numel].reshape(p.shape)\n",
    "        p.data.copy_(torch.from_numpy(chunk))\n",
    "        idx += numel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyswarms.single import GlobalBestPSO\n",
    "\n",
    "def make_fitness(X_train, y_train, device=\"cpu\"):\n",
    "    model = LSTMRegressor(n_features=X_train.shape[-1]).to(device)\n",
    "    dim = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    def fitness(pop):\n",
    "        losses = []\n",
    "        criterion = nn.MSELoss()\n",
    "        Xb = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "        yb = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "        for particle in pop:\n",
    "            unflatten_weights(model, particle)\n",
    "            preds = model(Xb).squeeze()\n",
    "            loss = criterion(preds, yb.squeeze()).item()\n",
    "            losses.append(loss)\n",
    "        return np.array(losses)\n",
    "\n",
    "    return fitness, dim, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pso_optimize(\n",
    "    fitness_func,\n",
    "    dim,\n",
    "    n_particles=30,\n",
    "    n_iters=100,\n",
    "    w=0.9,\n",
    "    c1=0.5,\n",
    "    c2=0.3,\n",
    "    bounds=None,\n",
    "    verbose=True\n",
    "):\n",
    "    if bounds is not None:\n",
    "        low, high = bounds\n",
    "        pos = np.random.uniform(low, high, (n_particles, dim))\n",
    "    else:\n",
    "        pos = np.random.randn(n_particles, dim) * 0.1\n",
    "    vel = np.zeros_like(pos)\n",
    "\n",
    "    fitness = fitness_func(pos)\n",
    "    pbest_pos   = pos.copy()\n",
    "    pbest_score = fitness.copy()\n",
    "    gbest_idx   = np.argmin(pbest_score)\n",
    "    gbest_pos   = pbest_pos[gbest_idx].copy()\n",
    "    gbest_score = pbest_score[gbest_idx]\n",
    "\n",
    "    for it in range(1, n_iters+1):\n",
    "        r1 = np.random.rand(n_particles, dim)\n",
    "        r2 = np.random.rand(n_particles, dim)\n",
    "\n",
    "        vel = (\n",
    "            w * vel\n",
    "            + c1 * r1 * (pbest_pos - pos)\n",
    "            + c2 * r2 * (gbest_pos - pos)\n",
    "        )\n",
    "\n",
    "        pos = pos + vel\n",
    "\n",
    "        if bounds is not None:\n",
    "            pos = np.clip(pos, low, high)\n",
    "\n",
    "        fitness = fitness_func(pos)\n",
    "\n",
    "        better_mask = fitness < pbest_score\n",
    "        pbest_pos[better_mask]   = pos[better_mask]\n",
    "        pbest_score[better_mask] = fitness[better_mask]\n",
    "\n",
    "        current_best_idx = np.argmin(pbest_score)\n",
    "        if pbest_score[current_best_idx] < gbest_score:\n",
    "            gbest_score = pbest_score[current_best_idx]\n",
    "            gbest_pos   = pbest_pos[current_best_idx].copy()\n",
    "\n",
    "        if verbose and it % 10 == 0:\n",
    "            print(f\"Iter {it:3d}/{n_iters},  gbest MSE = {gbest_score:.6f}\")\n",
    "\n",
    "    return gbest_score, gbest_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def flatten_gradients(model):\n",
    "    return np.concatenate([\n",
    "        p.grad.detach().cpu().numpy().ravel()\n",
    "        for p in model.parameters()\n",
    "    ])\n",
    "\n",
    "def hybrid_pso_optimize(\n",
    "    model,\n",
    "    X_train, y_train,\n",
    "    dim,\n",
    "    n_particles=30,\n",
    "    n_iters=100,\n",
    "    w=0.9,\n",
    "    c1=0.5,\n",
    "    c2=0.3,\n",
    "    alpha=0.01,\n",
    "    bounds=None,\n",
    "    verbose=True\n",
    "):\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    if bounds is not None:\n",
    "        low, high = bounds\n",
    "        pos = np.random.uniform(low, high, (n_particles, dim))\n",
    "    else:\n",
    "        pos = np.random.randn(n_particles, dim)*0.1\n",
    "    vel = np.zeros_like(pos)\n",
    "\n",
    "    def eval_fitness(pop):\n",
    "        out = []\n",
    "        for p in pop:\n",
    "            unflatten_weights(model, p)\n",
    "            with torch.no_grad():\n",
    "                preds = model(X_train).squeeze()\n",
    "            loss = criterion(preds, y_train.squeeze()).item()\n",
    "            out.append(loss)\n",
    "        return np.array(out)\n",
    "\n",
    "    fitness = eval_fitness(pos)\n",
    "    pbest_pos   = pos.copy()\n",
    "    pbest_score = fitness.copy()\n",
    "    gidx        = np.argmin(pbest_score)\n",
    "    gbest_pos   = pbest_pos[gidx].copy()\n",
    "    gbest_score = pbest_score[gidx]\n",
    "\n",
    "    for it in range(1, n_iters+1):\n",
    "        r1 = np.random.rand(n_particles, dim)\n",
    "        r2 = np.random.rand(n_particles, dim)\n",
    "\n",
    "        grads = []\n",
    "        for p in pos:\n",
    "            unflatten_weights(model, p)\n",
    "            model.zero_grad()\n",
    "            preds = model(X_train).squeeze()\n",
    "            loss  = criterion(preds, y_train.squeeze())\n",
    "            loss.backward()\n",
    "            grads.append(flatten_gradients(model))\n",
    "        grads = np.array(grads)\n",
    "\n",
    "        vel = (\n",
    "            w * vel\n",
    "            + c1 * r1 * (pbest_pos - pos)\n",
    "            + c2 * r2 * (gbest_pos - pos)\n",
    "            - alpha * grads\n",
    "        )\n",
    "\n",
    "        pos = pos + vel\n",
    "        if bounds is not None:\n",
    "            pos = np.clip(pos, low, high)\n",
    "\n",
    "        fitness = eval_fitness(pos)\n",
    "\n",
    "        better = fitness < pbest_score\n",
    "        pbest_pos[better]   = pos[better]\n",
    "        pbest_score[better] = fitness[better]\n",
    "\n",
    "        idx = np.argmin(pbest_score)\n",
    "        if pbest_score[idx] < gbest_score:\n",
    "            gbest_score = pbest_score[idx]\n",
    "            gbest_pos   = pbest_pos[idx].copy()\n",
    "\n",
    "        if verbose and it % 10 == 0:\n",
    "            print(f\"Iter {it:3d}/{n_iters}, gbest MSE = {gbest_score:.6f}\")\n",
    "\n",
    "    return gbest_score, gbest_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# options = {\"c1\": 0.5, \"c2\": 0.3, \"w\": 0.9}\n",
    "\n",
    "# results = {}\n",
    "# for d, group in district_groups.items():\n",
    "#     # 1) Prepare sequences and scaler\n",
    "#     X_np, y_np, scaler = make_sequences(group, seq_len=10)\n",
    "\n",
    "#     # 2) Init model on device and compute parameter‐space dim\n",
    "#     model = LSTMRegressor(n_features=X_np.shape[-1]).to(device)\n",
    "#     dim   = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "#     # 3) Convert data to torch tensors\n",
    "#     Xb = torch.tensor(X_np, dtype=torch.float32).to(device)\n",
    "#     yb = torch.tensor(y_np, dtype=torch.float32).to(device)\n",
    "\n",
    "#     # 4) Hybrid PSO+gradient optimization\n",
    "#     best_cost, best_pos = hybrid_pso_optimize(\n",
    "#         model, Xb, yb, dim,\n",
    "#         n_particles=30,\n",
    "#         n_iters=100,\n",
    "#         w=options[\"w\"],\n",
    "#         c1=options[\"c1\"],\n",
    "#         c2=options[\"c2\"],\n",
    "#         alpha=0.01,         # gradient step size\n",
    "#         bounds=(-1, 1),\n",
    "#         verbose=False\n",
    "#     )\n",
    "\n",
    "#     # 5) Load the best‐found weights\n",
    "#     unflatten_weights(model, best_pos)\n",
    "\n",
    "#     # 6) Store results\n",
    "#     results[d] = {\"model\": model, \"scaler\": scaler, \"train_mse\": best_cost}\n",
    "#     print(f\"District {d}: train MSE = {best_cost:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "options = {\"c1\": 0.5, \"c2\": 0.3, \"w\": 0.9}\n",
    "\n",
    "results = {}\n",
    "for d, group in district_groups.items():\n",
    "    # 1) build full sequences\n",
    "    X, y, scaler = make_sequences(group, seq_len=10)\n",
    "\n",
    "    # 2) split into train & test by time\n",
    "    split_idx = int(len(X) * 0.8)     # first 80% for training\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "    # 3) convert to torch and move to device\n",
    "    Xb_train = torch.tensor(X_train, dtype=torch.float32, device=device)\n",
    "    yb_train = torch.tensor(y_train, dtype=torch.float32, device=device)\n",
    "    Xb_test  = torch.tensor(X_test,  dtype=torch.float32, device=device)\n",
    "    yb_test  = torch.tensor(y_test,  dtype=torch.float32, device=device)\n",
    "\n",
    "    # 4) create & train model via hybrid PSO\n",
    "    model = LSTMRegressor(n_features=X.shape[-1]).to(device)\n",
    "    dim   = sum(p.numel() for p in model.parameters())\n",
    "    best_cost, best_pos = hybrid_pso_optimize(\n",
    "        model, Xb_train, yb_train, dim,\n",
    "        n_particles=30, n_iters=100,\n",
    "        w=0.9, c1=0.5, c2=0.3, alpha=0.01,\n",
    "        bounds=(-1,1), verbose=True\n",
    "    )\n",
    "    unflatten_weights(model, best_pos)\n",
    "\n",
    "    # 5) evaluate on test set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_scaled = model(Xb_test).cpu().numpy()\n",
    "    # inverse‐scale predictions & ground truth\n",
    "    y_pred = scaler.inverse_transform(y_pred_scaled)\n",
    "    y_true = scaler.inverse_transform(y_test)\n",
    "    test_mse = ((y_pred - y_true)**2).mean()\n",
    "\n",
    "    # 6) store and report\n",
    "    results[d] = {\n",
    "      \"model\": model,\n",
    "      \"scaler\": scaler,\n",
    "      \"train_mse\": best_cost,\n",
    "      \"test_mse\": test_mse\n",
    "    }\n",
    "    print(f\"District {d}: train MSE = {best_cost:.4f}, test MSE = {test_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "District 1: train MSE = 0.0101\n",
      "District 2: train MSE = 0.0212\n",
      "District 3: train MSE = 0.0169\n",
      "District 4: train MSE = 0.0109\n",
      "District 5: train MSE = 0.0179\n",
      "District 6: train MSE = 0.0224\n",
      "District 7: train MSE = 0.0100\n",
      "District 8: train MSE = 0.0235\n",
      "District 9: train MSE = 0.0192\n",
      "District 10: train MSE = 0.0184\n",
      "District 11: train MSE = 0.0254\n",
      "District 12: train MSE = 0.0117\n",
      "District 13: train MSE = 0.0164\n",
      "District 15: train MSE = 0.0202\n",
      "District 16: train MSE = 0.0281\n",
      "District 17: train MSE = 0.0077\n",
      "District 18: train MSE = 0.0141\n",
      "District 19: train MSE = 0.0106\n",
      "District 20: train MSE = 0.0296\n",
      "District 21: train MSE = 0.0182\n",
      "District 22: train MSE = 0.0244\n",
      "District 23: train MSE = 0.0151\n",
      "District 24: train MSE = 0.0152\n",
      "District 25: train MSE = 0.0204\n",
      "District 26: train MSE = 0.0191\n",
      "District 27: train MSE = 0.0181\n",
      "District 28: train MSE = 0.0073\n",
      "District 29: train MSE = 0.0082\n",
      "District 30: train MSE = 0.0212\n",
      "District 31: train MSE = 0.0141\n",
      "District 32: train MSE = 0.0107\n",
      "District 33: train MSE = 0.0248\n",
      "District 34: train MSE = 0.0203\n",
      "District 35: train MSE = 0.0160\n",
      "District 36: train MSE = 0.0244\n",
      "District 37: train MSE = 0.0161\n",
      "District 38: train MSE = 0.0204\n"
     ]
    }
   ],
   "source": [
    "# options = {\"c1\": 0.5, \"c2\": 0.3, \"w\": 0.9}\n",
    "\n",
    "# results = {}\n",
    "# for d, group in district_groups.items():\n",
    "#     X, y, scaler = make_sequences(group, seq_len=10)\n",
    "#     fitness, dim, model = make_fitness(X, y)\n",
    "\n",
    "#     # library implemented PSO\n",
    "#     opt = GlobalBestPSO(n_particles=30, dimensions=dim, options=options)\n",
    "#     best_cost, best_pos = opt.optimize(fitness, iters=100, verbose=False)\n",
    "\n",
    "#     # hand made PSO\n",
    "#     # best_cost, best_pos = pso_optimize(\n",
    "#     #     fitness,\n",
    "#     #     dim,\n",
    "#     #     n_particles=30,\n",
    "#     #     n_iters=100,\n",
    "#     #     w=0.9,\n",
    "#     #     c1=0.5,\n",
    "#     #     c2=0.3,\n",
    "#     #     bounds=(-1, 1),    # optional: clamp weights to [-1, 1]\n",
    "#     #     verbose=True\n",
    "#     # )\n",
    "    \n",
    "#     unflatten_weights(model, best_pos)\n",
    "#     results[d] = {\"model\": model, \"scaler\": scaler, \"train_mse\": best_cost}\n",
    "#     print(f\"District {d}: train MSE = {best_cost:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
